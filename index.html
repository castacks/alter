<!DOCTYPE html>
<html>
<head>
  <!-- <link rel = "icon" href = "assets/title_image.png" type = "image/x-icon"> -->
        
  <meta charset="utf-8">
  <meta name="description"
        content="ALTER">
  <meta name="keywords" content="offroad driving, perception, online learning, traversability">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- <meta property="og:image" content="assets/Thumbnail.png" />         -->
  <title>Learning-on-the-Drive: Self-supervised Adaptive Long-range Perception for High-speed Offroad Driving</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ERWFHDG4GX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-ERWFHDG4GX');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

</head>
<!-- <body>
<h1>Learning-on-the-Drive: Self-supervised Adaptive Long-range Perception for High-speed Offroad Driving</h1>
</body> -->

<body>
      <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
          <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>
        </div>
        <div class="navbar-menu">
          <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
            <a class="navbar-item" href="https://theairlab.org/research/">
            <span class="icon">
                <i class="fas fa-home"></i>
            </span>
            </a>
          </div>
      
        </div>
      </nav>
      
      
      <section class="hero">
        <div class="hero-body">
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <span class="title is-1 publication-title">Learning-on-the-Drive</span><h2 class="title is-6 publication-title">Adaptive Long-range Perception for High-speed Offroad Driving</h2> 
                <div class="is-size-6 publication-authors">
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/eric-chen-2b8726208/">Eric Chen</a>*<sup>1,2</sup>,</span>
                  <span class="author-block">
                    <a href="https://cherieho.com/">Cherie Ho</a>*<sup>1</sup></span>,
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/maulimov/">Mukhtar Maulimov</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://sairlab.org/team/chenw/">Chen Wang</a><sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="https://theairlab.org/team/sebastian/">Sebastian Scherer</a><sup>1</sup>,</span>
      
            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup>Carnegie Mellon University</span>
              <span class="author-block"><sup>2</sup>Harvey Mudd College</span>
              <span class="author-block"><sup>3</sup>University at Buffalo</span>
            </div>
                <div class="is-size-7 publication-authors">
                  <span class="author-block"><sup>*</sup>Co-first authors</span>
                </div>
                
                <div class="column has-text-centered">
                  <div class="publication-links">
                    <!-- PDF Link. -->
                    <span class="link-block">
                      <a href=""
                         class="external-link button is-normal ">
                        <span class="icon">
                            <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                    <span class="link-block">
                      <a href=""
                         class="external-link button is-normal ">
                        <span class="icon">
                            <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>
                    <!-- Video Link. -->
                    <span class="link-block">
                      <a href=""
                         class="external-link button is-normal ">
                        <span class="icon">
                            <i class="fab fa-youtube"></i>
                        </span>
                        <span>Video</span>
                      </a>
                    </span>
                    <!-- Thread Link. -->
                    <span class="link-block">
                      <a href=""
                         class="external-link button is-normal ">
                        <span class="icon">
                            <i class="fab fa-twitter"></i>
                        </span>
                        <span>Thread</span>
                      </a>
                    </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>
      
      <section class="hero">
          <div class="hero-body">
            <h2 class="subtitle has-text-centered">
              <span>Offroad driving depends on robust long range perception. </span>
            </h2>
              <div class="container is-max-desktop">
                <div class="columns is-centered large-content">
                  <figure class="image large-content">
                    <img src="data/Intro_video.gif" alt="My GIF">
                  </figure
                </div>
              </div>
              <h2 class="subtitle has-text-centered">
                <span>We present ALTER, an adaptive self-supervised framework designed for long range perception.</span>
              </h2>
          </div>
      </section>
      
      <section class="hero is-small is-light">
        <div class="hero-body">
        <div class="container is-max-desktop">
          <!-- Abstract. -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Abstract</h2>
              <div class="content has-text-justified">
                <p>
                  Autonomous offroad driving is essential for applications like
                   emergency rescue, military operations, and agriculture.
                   Despite progress, systems struggle with high-speed vehicles 
                   exceeding 10m/s due to the need  for accurate long-range (>
                    50m) perception for safe navigation. Current approaches ar
                    e limited by sensor constraints; LiDAR-based methods offer
                     precise short-range data but are noisy beyond 30m, while 
                     visual models provide dense long-range measurements but 
                     falter with unseen scenarios. To overcome these issues, 
                     we introduce ALTER, a learning-on-the-drive perception 
                     framework that leverages both sensor types. ALTER uses a
                      self-supervised visual model to learn from near-range 
                      LiDAR measurements in real-time, improving long-range 
                      prediction in new environments without manual labeling. 
                      It also includes a model selection module for better sensor 
                      failure response and adaptability to known environments.
                       Testing in two real-world settings showed on average 43.4%
                       better traversability prediction than LiDAR-only and 
                       164% over non-adaptive state-of-the-art (SOTA) visual 
                       semantic methods after 45 seconds of online learning.
                </p>
              </div>
            </div>
          </div>
          </div>
          <!-- Paper video. -->
          <!-- <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Video</h2>
              <div class="publication-video">
                <a id="overview_video"></a>
                <iframe
                  src="./data/">
                </iframe>
              </div>
            </div>
          </div>
        </div> -->
          <!--/ Paper video. -->
      </section>
      
      <section class="hero">
        <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <div class="column is-three-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe width="560" height="315" src="https://www.youtube.com/embed/jWLI-OFp3qU?si=Ba4g6Io7ywE7wmqA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </div>
          </div>
        </div>
        </div>
      </section>

      <section class="hero is-small is-light">
        <div class="hero-body">
        <div class="columns is-centered">
          <div class="column">
            <h2 class="title is-4 has-text-centered">LiDAR and Cameras are invaluable sensors for perception,
              but they are not enough on their own.
            </h2>
            <!-- put two images side by side Each image also has their individual subtitles -->
          <div class="container is-max-desktop">  
            <div class="columns large-content">
              <div class="column">
                  <figure class="image">
                      <img src="data/lidar_sensor.png" alt="LiDAR sensor image shows sparse data at far range">
                      <figcaption class="title is-5 has-text-centered">LiDAR-based Sensing</figcaption>
                    <ul class="list-no-bullets">
                      <li class="pro">+ Robust to appearance changes</li>
                      <li class="con">- Sparse measurement at far-range</li>
                    </ul>
                  </figure>
              </div>
              <div class="column">
                  <figure class="image">
                      <img src="data/camera-sensor.png" alt="Vision models provide labels for every pixel of the terrain but is incorrect on certain regions">
                      <figcaption class="title is-5 has-text-centered">Camera-based</figcaption>
                      <ul class="list-no-bullets">
                        <li class="pro">+ Dense measurement at far-range</li>
                        <li class="con">- Fail when out of distribution</li>
                      </ul>
                  </figure>
              </div>
          </div>
        </div>
          </div>
        </div>
        </div>
        </div>
      </section>

      
      <section class="hero is-small is-light">
        <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <h2 class="title is-4">ALTER combines LiDAR and Camera sensors to generate labeled data online, offering the best of both worlds</h2>
          </div>
        </div>

        <div class="container is-max-desktop">
          <div class="large-content">
            <figure class="image">
              <img src="data/combine_lidar_and_camera.png" alt="LiDAR sensor image shows sparse data at far range">
              <figcaption>Each training example consists of an image taken at time t and the accumulated LiDAR-based traversability labels at time t+n.</figcaption>
            </figure>
          </div>
        
          <div class="columns">
            <div class="column">
              <div>&emsp; ALTER trains a visual model using camera images and LiDAR derived labels for supervision.
                To generate the traversability labels, point cloud data is spatially processed, factoring in height and surface roughness.
                The framework is easily extensible to include other point cloud processing methods.
              </div>
              <div>&emsp; A single sweep of the LiDAR from one location does not provide an accurate label. Thus,
                ALTER uses accumulated point clouds gather over N seconds for each image label. Notice in the gif,
               the point clouds become more dense as the vehicle travels in that direction. This accumulation
                process increases the quality and quantity of labeled pixels.
              </div>
            </div>
            <div class="column">
              <div class="content">
                <video id="s2_recon" autoplay controls muted loop playsinline height="100%">
                  <source src="./data/accumulate_lidar.mp4"
                          type="video/mp4">
                </video>
                <figcaption>Accumulation increases the density and range of point clouds.</figcaption>

              </div>
            </div>
          </div>

          <h2 class="title is-4 is-centered has-text-centered">AlTER uses this online labeled data to train a new visual model every 10 seconds,
            ready for inference in as little as 15 seconds after data collection.</h2>
          
            <div class="large-content">
              <figure class="image">
                <img src="data/alter_approach_0314.png" alt="Alter approach figure">
                <!-- Our adaptive approach estimates traversability from images using models trained online with new LiDAR data.  -->
                <figcaption>
                  Online Labeling: We accumulate LiDAR features to label their costs in 3D. These labels are projected onto the image plane to form
                  training labels.
                </figcaption>
                <figcaption>
                  Online Learning: ALTER picks the most similar model or makes a new model for training to adapt to new appearances.
                </figcaption>
                <figcaption>
                  Online Inference: We pick the best inference model given image similarity or LiDAR to predict pixel-wise fine-grained traversability scores
                </figcaption>
              </figure>
            </div>
        </div>
      </section>
      

      <section class="hero is-small is-light">
        <div class="hero-body">
          <div class="columns is-centered has-text-centered">
            <div class="column">
              <h2 class="title is-4">Results: Self-Supervised Labels</h2>
              <p></p>
          </div>
        </div>

        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column">
              <div class="large-content">
                <video id="s2_recon" autoplay controls muted loop playsinline height="100%">
                  <source src="./data/self_supervised_labels (2).mp4"
                          type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>
      </section>

      <section class="hero is-small is-light">
        <div class="hero-body">
          <div class="columns is-centered has-text-centered">
            <div class="column">
              <h2 class="title is-4">Results: Visual Model Performance</h2>
              <p></p>
          </div>
        </div>
        <div class="container is-max-desktop">
          <div class="large-content">
            <figure class="image">
              <img src="data/alter_cont_seg_0314_2.png" alt="Vision models provide labels for every pixel of the terrain but is incorrect on certain regions">
              <figcaption>
                <b>Qualitative Comparison to Hand-labels after 45s of Adaptation:</b> ALTER enables more accurate differentiation of the classes at
long-range. Compared to non-adaptive methods, our adaptive method provides more robust predictions when deployed in a new environment.
              </figcaption>
            </figure>
          </div>
            
            <div class="hero-body">
              <div class="columns is-centered has-text-centered">
                <div class="column">
                  <h2 class="title is-6">ALTER's performance in an unseen offroad sequence.</h2>
                </div>
              </div>
            </div>
            <div class="large-content" >
              <video id="s2_recon" autoplay controls muted loop playsinline height="100%">
                  <source src="./data/forest_result_video.mp4" type="video/mp4">
              </video>
          </div>
        </div>
      </section>

      <section class="hero is-small is-light">
        <div class="hero-body">
          <div class="columns is-centered has-text-centered">
            <div class="column">
              <h2 class="title is-6">Besides increasing perception range, ALTER also improves robustness by detecting out of distribution scenarios and degrading to LiDAR.</h2>
            </div>
          </div>
        </div>
        <div class="container is-max-desktop">
          <div class="large-content">
            <figure class="image">
              <img src="data/alter_dropout_0315.png" alt="Vision models provide labels for every pixel of the terrain but is incorrect on certain regions">
              <figcaption>
               ALTER uses its model selection process to detect view drops and safely degrades to a secondary sensor (LiDAR).
              </figcaption>
            </figure>
          </div>
        </div>
      </section>


      
      <script type="text/javascript">
        $(function() {
        var screenWidth = $(window).width();
        if (screenWidth >= 800) {
          $('#gpt-video-1').attr('autoplay', 'autoplay');
        }
        if (screenWidth >= 800) {
          $('#gpt-video-2').attr('autoplay', 'autoplay');
        }
        if (screenWidth >= 800) {
          $('#click-query-icl').attr('autoplay', 'autoplay');
        }
      });
      </script>
      
      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title is-3">BibTeX</h2>
          <pre><code>
            @misc{chen2023learning,
                  title={Learning-on-the-Drive: Self-supervised Adaptation of Visual Offroad Traversability Models}, 
                  author={Eric Chen and Cherie Ho and Mukhtar Maulimov and Chen Wang and Sebastian Scherer},
                  year={2023},
                  eprint={2306.15226},
                  archivePrefix={arXiv},
                  primaryClass={cs.RO},
                  url={https://arxiv.org/abs/2306.15226}, 
            }
          </code></pre>
        </div>
      </section>
      
      
      <style>
        #blocks {
            width:100%;
            height:60px;
            margin:0 auto;
        }
        #block1 {
            height:33.33%;
            width:30%;
            float: left;
        }
        #block2 {
            height:33.33%;
            width:40%;
            float: left;
        }
        #block3 {
            height:33.33%;
            width:30%;
            float: right;
        }
      </style>
      
      <footer class="footer">
          <div class="container">
              <div id="blocks">
                <div id="block1">
                  <a href="https://theairlab.org/">
                    <img src="static/images/Horizontal@2x.png" alt="AirLab Logo" style="width:60%;">
                  </a>
                </div>
                <div id="block2">
                  <center>
                    <a class="button is-text" itemprop="linkedin" href="https://www.linkedin.com/company/10629296/admin/feed/posts/" target="_blank">
                      <i class="fab fa-linkedin fa-lg" style="height:100%;"></i>
                    </a>
                    <a class="button is-text" itemprop="facebook" href="https://www.facebook.com/airlabcmu/" target="_blank">
                      <i class="fab fa-facebook fa-lg" style="height:100%;"></i>
                    </a>
                    <a class="button is-text" itemprop="twitter" href="https://www.twitter.com/airlabcmu/" target="_blank">
                      <i class="fab fa-twitter fa-lg"></i>
                    </a>
                    <a class="button is-text" itemprop="medium" href="https://medium.com/airlabcmu" target="_blank">
                      <i class="fab fa-medium fa-lg"></i>
                    </a>
                    <a class="button is-text" itemprop="github" href="https://github.com/castacks" target="_blank">
                      <i class="fab fa-github fa-lg"></i>
                    </a>
                    <a class="button is-text" itemprop="bitbucket" href="https://bitbucket.org/castacks/" target="_blank">
                      <i class="fab fa-bitbucket fa-lg"></i>
                    </a>
                    <br>
                    <br>
                    <p class="" style="font-size: .65rem;">This webpage was adapted from the Nerfies templates, 
                        which is licensed under a 
                        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>. 
                        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
                          <img alt="Creative Commons License" style="border-width:0; height: 0.65rem;" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" />
                        </a>
                    </p>
                    <p class="" style="font-size: .65rem;">If you use the <a href="https://github.com/castacks/visafe">source code</a> of this webpage,
                      please also link back to the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies source code</a> in your footer.</p>
                  </center>
                </div>
                <div id="block3">
                  <a href="https://www.ri.cmu.edu/">
                    <img src="static/images/riLogo2019.svg" alt="RI Logo" style="float: right;">
                  </a>
                </div>
              </div>
          </div>
      </footer>
      
      
      </body>
</html>
